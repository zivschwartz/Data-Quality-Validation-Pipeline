{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/a1/2ab46a175c916b0149ccb9edc06202bce6365455779fa251c1f59a4c7806/tensorflow-2.0.0-cp36-cp36m-macosx_10_11_x86_64.whl (102.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 102.7MB 169kB/s ta 0:00:011   19% |██████▎                         | 20.0MB 3.2MB/s eta 0:00:26    22% |███████▏                        | 22.9MB 8.2MB/s eta 0:00:10    29% |█████████▋                      | 30.7MB 4.7MB/s eta 0:00:16\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /Applications/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.16.1)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/c2/08685ae86ee1f5a45839e779726640ba981e629da2999ebce17e62639c9a/protobuf-3.10.0-cp36-cp36m-macosx_10_9_intel.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /Applications/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K    100% |████████████████████████████████| 450kB 6.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /Applications/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.29.0)\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.8MB 2.4MB/s ta 0:00:011    47% |███████████████▏                | 1.8MB 5.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/a0/ab76a2314e06356346ab411f7797debde31cd89bc5f2a4541a5955fe5ff4/grpcio-1.25.0-cp36-cp36m-macosx_10_9_x86_64.whl (2.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.8 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (36.5.0.post20170921)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 7.0MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Applications/anaconda3/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.12.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /Applications/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow) (2.7.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Applications/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.2)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /Applications/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.0)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Applications/anaconda3/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.4)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 4.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /Applications/anaconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Applications/anaconda3/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Applications/anaconda3/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Building wheels for collected packages: absl-py, gast, opt-einsum, wrapt, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srlaad@us.ibm.com/Library/Caches/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srlaad@us.ibm.com/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srlaad@us.ibm.com/Library/Caches/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srlaad@us.ibm.com/Library/Caches/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/srlaad@us.ibm.com/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast opt-einsum wrapt termcolor\n",
      "\u001b[31mgoogle-auth 1.7.1 has requirement setuptools>=40.3.0, but you'll have setuptools 36.5.0.post20170921 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 2.0.1 has requirement setuptools>=41.0.0, but you'll have setuptools 36.5.0.post20170921 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, protobuf, tensorflow-estimator, absl-py, gast, opt-einsum, markdown, google-auth, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, google-pasta, wrapt, termcolor, astor, keras-applications, tensorflow\n",
      "  Found existing installation: google-auth 1.6.1\n",
      "    Uninstalling google-auth-1.6.1:\n",
      "      Successfully uninstalled google-auth-1.6.1\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mCannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a6d7e53fe47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_quality\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from data_quality import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import time\n",
    "import warnings\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_flights, dirty_flights = partition_data_files('tmp/FLIGHTS/*.csv')\n",
    "clean_fb, dirty_fb = partition_data_files('tmp/FBPosts/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@description: this is the same function from assignment2; infers schema from a given csv file\n",
    "@params: csv_file - string csv filepath to read\n",
    "         column_names - string list, names of the columns along the csv header\n",
    "@return: schema - inferred schema\n",
    "'''\n",
    "def infer_schema_from_csv(csv_file, column_names):\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file, column_names=column_names)\n",
    "    \n",
    "    #tfdv.visualize_statistics(data_stats)\n",
    "    schema = tfdv.infer_schema(statistics=data_stats)\n",
    "    \n",
    "    return schema \n",
    "\n",
    "'''\n",
    "@description: same function from assignment 2; determines whether the new csv file \n",
    "              has anomalies against the schema\n",
    "@params: csv_file - new csv filepath to compare against schema\n",
    "         schema - schema against which to compare the new csv file\n",
    "@return: True - if there are anomalies\n",
    "          False - if there are no anomalies\n",
    "'''\n",
    "def has_anomalies(csv_file, schema, environment='TRAINING'):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                   column_names=cols, stats_options=options)\n",
    "    \n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    anomalies = tfdv.validate_statistics(statistics=data_stats, schema=schema)\n",
    "    \n",
    "    #tfdv.display_anomalies(anomalies)\n",
    "\n",
    "    if len(anomalies.anomaly_info) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_num_anomalies(csv_file, schema, environment='TRAINING'):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                   column_names=cols, stats_options=options)\n",
    "    \n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    anomalies = tfdv.validate_statistics(statistics=data_stats, schema=schema, environment=environment)\n",
    "    \n",
    "    #tfdv.display_anomalies(anomalies)\n",
    "\n",
    "    return len(anomalies.anomaly_info)\n",
    "\n",
    "'''\n",
    "@description: takes the current csv file and updates the passed in schema to include its values as \"valid\"\n",
    "@params: csv_file - new csv file to add to the schema\n",
    "         schema - schema to be updated\n",
    "@return: updated_schema - new schema with anomalies of csv_file param added\n",
    "'''\n",
    "def update_schema(csv_file, schema):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    new_batch_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                        column_names=cols, stats_options = options)\n",
    "\n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    updated_schema = tfdv.update_schema(schema, new_batch_stats)\n",
    "    #tfdv.display_schema(schema=updated_schema)\n",
    "\n",
    "    return updated_schema\n",
    "\n",
    "'''\n",
    "@description: updates the schema for num_files number of files\n",
    "@params: all_filenames - string list of all csv filenames\n",
    "         num_files - the number of files we want to include in our \"acceptable\" schema\n",
    "@return: updated_schema - new schema with anomalies of all num_files number of csv files added\n",
    "'''\n",
    "def acceptable_schema(training_files):\n",
    "    successful = True\n",
    "    \n",
    "    for i, file in enumerate(training_files):\n",
    "        #print(\"Reading file: \", file)\n",
    "        \n",
    "        #for the first file read in, infer its schema\n",
    "        if i == 0:\n",
    "            columns = list(pd.read_csv(file, error_bad_lines=False).columns).sort()\n",
    "            schema = infer_schema_from_csv(file, columns)\n",
    "        else:\n",
    "            schema = update_schema(file, schema) #updated schema\n",
    "            #sanity check to make sure this is working\n",
    "            if has_anomalies(file, schema):\n",
    "                print('Unsuccessful schema update on file number {}: {}'.format(i,file))\n",
    "                successful = False\n",
    "                break\n",
    "                \n",
    "    if successful:\n",
    "        print(\"Schema successfully updated for all {} files\".format(len(training_files)))\n",
    "        \n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(train_batch, test_batch):\n",
    "    #set all flags (this is bc what we need to do with TFDV depends on the type of data we're dealing with)\n",
    "    flights = False\n",
    "    fb = False\n",
    "    dirty = False\n",
    "    #if this is flights data\n",
    "    if 'FLIGHTS' in test_batch[0]: flights = True\n",
    "    #if this is facebook data    \n",
    "    if 'FBPosts' in test_batch[0]: fb = True\n",
    "        \n",
    "    #if the test batch is a dirty file\n",
    "    if 'dirty' in test_batch[0]: dirty = True\n",
    "    \n",
    "    ######## now work on updating the schema ####### \n",
    "    #update schema with all the files in the train batch\n",
    "    updated_schema = acceptable_schema(training_files)\n",
    "    \n",
    "    #make sure TFDV knows the columns that are only in the clean + not the dirty data\n",
    "    updated_schema.default_environment.append('TRAINING')\n",
    "    updated_schema.default_environment.append('TESTING')\n",
    "    \n",
    "    #specify that 'for_key' feature is not in TESTING environment\n",
    "    if dirty and flights:\n",
    "        tfdv.get_feature(updated_schema, 'for_key').not_in_environment.append('TESTING')\n",
    "        \n",
    "    if dirty and fb:\n",
    "        ### do similar thing here ###\n",
    "        pass\n",
    "    \n",
    "\n",
    "    #get the number of anomalies for the test batch prior to relaxing constraints\n",
    "    if dirty:\n",
    "        num_anomalies = get_num_anomalies(test_batch[0], updated_schema, environment='TESTING')\n",
    "    else: \n",
    "        #if it's a clean file, default environment is TRAINING\n",
    "        num_anomalies = get_num_anomalies(test_batch[0], updated_schema)\n",
    "    \n",
    "    #relax constraints for each type of data\n",
    "    if flights:\n",
    "        tfdv.get_feature(updated_schema,'ArrivalGate').distribution_constraints.min_domain_mass = 0.8\n",
    "        tfdv.get_feature(updated_schema,'DepartureGate').distribution_constraints.min_domain_mass = 0.8\n",
    "    if fb:\n",
    "        ### do similar thing here ###\n",
    "        pass\n",
    "    \n",
    "    #get anomalies after having relaxed constraints\n",
    "    if dirty:\n",
    "        adjusted_num_anomalies = get_num_anomalies(test_batch[0], updated_schema, environment='TESTING')\n",
    "    else:\n",
    "        adjusted_num_anomalies = get_num_anomalies(test_batch[0], updated_schema)\n",
    "    \n",
    "    ######## determine whether acceptable or not ########\n",
    "    #my definition of \"acceptable\" is just if the relax constraints had any affect on the anomalies\n",
    "    #being present (so relaxing constraints should yield less anomalies after adjustment)\n",
    "    if adjusted_num_anomalies < num_anomalies: \n",
    "        return True #clean is marked as clean --> acceptable\n",
    "    else:\n",
    "        return False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema successfully updated for all 5 files\n",
      "False\n",
      "Finished in: 28.64 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(is_acceptable(clean_flights[0:5], dirty_flights[5:6]))\n",
    "print(\"Finished in: {:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_analysis = pd.DataFrame(columns=['train_type', 'batch_size', 'test_batch', 'clean_correct', 'dirty_correct'])\n",
    "counter = 0\n",
    "\n",
    "for train_type in ['rolling', 'increasing']:\n",
    "    for batch_size in range(1, 10):\n",
    "        for i in range(len(clean_fb)-batch_size):\n",
    "            row = analysis(i, train_type, clean_fb, dirty_fb, batch_size)\n",
    "            fb_analysis.loc[counter] = row\n",
    "            counter = counter + 1\n",
    "fb_analysis.to_csv('fb_batch_analysis_tfdv.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_accuracy = get_accuracy(fb_analysis)\n",
    "fb_accuracy.to_csv('fb_accuracy_tfdv.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_analysis = pd.DataFrame(columns=['train_type', 'batch_size', 'test_batch', 'clean_correct', 'dirty_correct'])\n",
    "counter = 0\n",
    "\n",
    "for train_type in ['rolling', 'increasing']:\n",
    "    for batch_size in range(1, 10):\n",
    "        for i in range(len(clean_flights)-batch_size):\n",
    "            row = analysis(i, train_type, clean_flights, dirty_flights, batch_size)\n",
    "            flights_analysis.loc[counter] = row\n",
    "            counter = counter + 1\n",
    "flights_analysis.to_csv('flights_batch_analysis_tfdv.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_accuracy = get_accuracy(flights_analysis)\n",
    "flights_accuracy.to_csv('flights_accuracy_tfdv.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch('facebook', fb_analysis, range(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch('flights', flights_analysis, range(1, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
