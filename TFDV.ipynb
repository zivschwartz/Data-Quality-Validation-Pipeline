{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from data_quality import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import time\n",
    "import warnings\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_flights, dirty_flights = partition_data_files('tmp/FLIGHTS/*.csv')\n",
    "clean_fb, dirty_fb = partition_data_files('tmp/FBPosts/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@description: this is the same function from assignment2; infers schema from a given csv file\n",
    "@params: csv_file - string csv filepath to read\n",
    "         column_names - string list, names of the columns along the csv header\n",
    "@return: schema - inferred schema\n",
    "'''\n",
    "def infer_schema_from_csv(csv_file, column_names):\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file, column_names=column_names)\n",
    "    \n",
    "    #tfdv.visualize_statistics(data_stats)\n",
    "    schema = tfdv.infer_schema(statistics=data_stats)\n",
    "    \n",
    "    return schema \n",
    "\n",
    "'''\n",
    "@description: same function from assignment 2; determines whether the new csv file \n",
    "              has anomalies against the schema\n",
    "@params: csv_file - new csv filepath to compare against schema\n",
    "         schema - schema against which to compare the new csv file\n",
    "@return: True - if there are anomalies\n",
    "          False - if there are no anomalies\n",
    "'''\n",
    "def has_anomalies(csv_file, schema, environment='TRAINING'):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                   column_names=cols, stats_options=options)\n",
    "    \n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    anomalies = tfdv.validate_statistics(statistics=data_stats, schema=schema)\n",
    "    \n",
    "    #tfdv.display_anomalies(anomalies)\n",
    "\n",
    "    if len(anomalies.anomaly_info) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_num_anomalies(csv_file, schema, environment='TRAINING'):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    data_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                   column_names=cols, stats_options=options)\n",
    "    \n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    anomalies = tfdv.validate_statistics(statistics=data_stats, schema=schema, environment=environment)\n",
    "    \n",
    "    #tfdv.display_anomalies(anomalies)\n",
    "\n",
    "    return len(anomalies.anomaly_info)\n",
    "\n",
    "'''\n",
    "@description: takes the current csv file and updates the passed in schema to include its values as \"valid\"\n",
    "@params: csv_file - new csv file to add to the schema\n",
    "         schema - schema to be updated\n",
    "@return: updated_schema - new schema with anomalies of csv_file param added\n",
    "'''\n",
    "def update_schema(csv_file, schema):\n",
    "    #get column names from passed in schema\n",
    "    cols = [f.name for f in schema.feature].sort()\n",
    "    \n",
    "    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "    new_batch_stats = tfdv.generate_statistics_from_csv(data_location=csv_file,\\\n",
    "                                                        column_names=cols, stats_options = options)\n",
    "\n",
    "    # Check eval data for errors by validating the eval data stats using the previously inferred schema\n",
    "    updated_schema = tfdv.update_schema(schema, new_batch_stats)\n",
    "    #tfdv.display_schema(schema=updated_schema)\n",
    "\n",
    "    return updated_schema\n",
    "\n",
    "'''\n",
    "@description: updates the schema for num_files number of files\n",
    "@params: all_filenames - string list of all csv filenames\n",
    "         num_files - the number of files we want to include in our \"acceptable\" schema\n",
    "@return: updated_schema - new schema with anomalies of all num_files number of csv files added\n",
    "'''\n",
    "def acceptable_schema(training_files):\n",
    "    successful = True\n",
    "    \n",
    "    for i, file in enumerate(training_files):\n",
    "        #print(\"Reading file: \", file)\n",
    "        \n",
    "        #for the first file read in, infer its schema\n",
    "        if i == 0:\n",
    "            columns = list(pd.read_csv(file, error_bad_lines=False).columns).sort()\n",
    "            schema = infer_schema_from_csv(file, columns)\n",
    "        else:\n",
    "            schema = update_schema(file, schema) #updated schema\n",
    "            #sanity check to make sure this is working\n",
    "            if has_anomalies(file, schema):\n",
    "                print('Unsuccessful schema update on file number {}: {}'.format(i,file))\n",
    "                successful = False\n",
    "                break\n",
    "                \n",
    "    if successful:\n",
    "        print(\"Schema successfully updated for all {} files\".format(len(training_files)))\n",
    "        \n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(train_batch, test_batch):\n",
    "    #set all flags (this is bc what we need to do with TFDV depends on the type of data we're dealing with)\n",
    "    flights = False\n",
    "    fb = False\n",
    "    dirty = False\n",
    "    #if this is flights data\n",
    "    if 'FLIGHTS' in test_batch[0]: flights = True\n",
    "    #if this is facebook data    \n",
    "    if 'FBPosts' in test_batch[0]: fb = True\n",
    "        \n",
    "    #if the test batch is a dirty file\n",
    "    if 'dirty' in test_batch[0]: dirty = True\n",
    "    \n",
    "    ######## now work on updating the schema ####### \n",
    "    #update schema with all the files in the train batch\n",
    "    updated_schema = acceptable_schema(training_files)\n",
    "    \n",
    "    #make sure TFDV knows the columns that are only in the clean + not the dirty data\n",
    "    updated_schema.default_environment.append('TRAINING')\n",
    "    updated_schema.default_environment.append('TESTING')\n",
    "    \n",
    "    #specify that 'for_key' feature is not in TESTING environment\n",
    "    if dirty and flights:\n",
    "        tfdv.get_feature(updated_schema, 'for_key').not_in_environment.append('TESTING')\n",
    "        \n",
    "    if dirty and fb:\n",
    "        ### do similar thing here ###\n",
    "        pass\n",
    "    \n",
    "\n",
    "    #get the number of anomalies for the test batch prior to relaxing constraints\n",
    "    if dirty:\n",
    "        num_anomalies = get_num_anomalies(test_batch[0], updated_schema, environment='TESTING')\n",
    "    else: \n",
    "        #if it's a clean file, default environment is TRAINING\n",
    "        num_anomalies = get_num_anomalies(test_batch[0], updated_schema)\n",
    "    \n",
    "    #relax constraints for each type of data\n",
    "    if flights:\n",
    "        tfdv.get_feature(updated_schema,'ArrivalGate').distribution_constraints.min_domain_mass = 0.8\n",
    "        tfdv.get_feature(updated_schema,'DepartureGate').distribution_constraints.min_domain_mass = 0.8\n",
    "    if fb:\n",
    "        ### do similar thing here ###\n",
    "        pass\n",
    "    \n",
    "    #get anomalies after having relaxed constraints\n",
    "    if dirty:\n",
    "        adjusted_num_anomalies = get_num_anomalies(test_batch[0], updated_schema, environment='TESTING')\n",
    "    else:\n",
    "        adjusted_num_anomalies = get_num_anomalies(test_batch[0], updated_schema)\n",
    "    \n",
    "    ######## determine whether acceptable or not ########\n",
    "    #my definition of \"acceptable\" is just if the relax constraints had any affect on the anomalies\n",
    "    #being present (so relaxing constraints should yield less anomalies after adjustment)\n",
    "    if adjusted_num_anomalies < num_anomalies: \n",
    "        return True #clean is marked as clean --> acceptable\n",
    "    else:\n",
    "        return False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema successfully updated for all 5 files\n",
      "False\n",
      "Finished in: 28.64 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(is_acceptable(clean_flights[0:5], dirty_flights[5:6]))\n",
    "print(\"Finished in: {:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
